{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5004d729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-05 09:28:35,769\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All classes and functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# === Cell 1: Imports and All Class/Function Definitions ===\n",
    "# ==============================================================================\n",
    "# (This cell contains all your imports and helper code)\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from torch import optim\n",
    "\n",
    "# Flower\n",
    "import flwr as fl\n",
    "\n",
    "# sklearn metrics + plotting\n",
    "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n",
    "                             confusion_matrix, roc_auc_score, classification_report)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# Configurable defaults\n",
    "# ---------------------------\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset that reads from HDF5 lazily\n",
    "# ---------------------------\n",
    "class H5RFShard(Dataset):\n",
    "    \"\"\"\n",
    "    Lazy HDF5 dataset for one client's shard.\n",
    "    \"\"\"\n",
    "    def __init__(self, h5_path: str, split: str = \"train\", downsample: int = 8, transform=None):\n",
    "        self.h5_path = h5_path\n",
    "        self.downsample = int(downsample)\n",
    "        self.transform = transform\n",
    "        try:\n",
    "            with h5py.File(self.h5_path, \"r\") as f:\n",
    "                self.length = f[\"y\"].shape[0]\n",
    "        except (IOError, OSError, FileNotFoundError) as e:\n",
    "            print(f\"Error opening HDF5 file {self.h5_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        with h5py.File(self.h5_path, \"r\") as f:\n",
    "            spec = f[\"x_spec\"][idx]      # shape [2,128,128]\n",
    "            iq = f[\"x_iq\"][idx]         # shape [2, L]\n",
    "            y = int(f[\"y\"][idx])\n",
    "            snr = None\n",
    "            if \"snr\" in f:\n",
    "                snr = f[\"snr\"][idx]\n",
    "\n",
    "        if self.downsample > 1:\n",
    "            iq = iq[:, :: self.downsample]\n",
    "\n",
    "        spec_t = torch.tensor(spec, dtype=torch.float32)\n",
    "        iq_t = torch.tensor(iq, dtype=torch.float32)\n",
    "        label_t = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            spec_t = self.transform(spec_t)\n",
    "\n",
    "        return {\"iq\": iq_t, \"spec\": spec_t, \"label\": label_t, \"snr\": snr}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Helper class for augmentations\n",
    "# ---------------------------\n",
    "class TransformedDataset(Dataset):\n",
    "    \"\"\"Applies a transform to a subset/dataset.\"\"\"\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        if self.transform:\n",
    "            sample[\"spec\"] = self.transform(sample[\"spec\"])\n",
    "        return sample\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Model: ResNet (spec) + 1D-CNN-Transformer (iq) fusion\n",
    "# ---------------------------\n",
    "class MultiModalNet(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.spec_net = models.resnet18(weights=None)\n",
    "        self.spec_net.conv1 = nn.Conv2d(2, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.spec_net.fc = nn.Identity()\n",
    "\n",
    "        self.iq_conv = nn.Sequential(\n",
    "            nn.Conv1d(2, 32, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=128, nhead=4, dim_feedforward=256,\n",
    "            dropout=0.1, activation=\"relu\", batch_first=True\n",
    "        )\n",
    "        self.iq_transformer = nn.TransformerEncoder(transformer_layer, num_layers=1)\n",
    "        \n",
    "        self.iq_pool_flat = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 + 128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, iq: torch.Tensor, spec: torch.Tensor) -> torch.Tensor:\n",
    "        f_spec = self.spec_net(spec)\n",
    "        x_iq = self.iq_conv(iq)\n",
    "        x_iq = x_iq.permute(0, 2, 1)\n",
    "        x_iq = self.iq_transformer(x_iq)\n",
    "        x_iq = x_iq.permute(0, 2, 1)\n",
    "        f_iq = self.iq_pool_flat(x_iq)\n",
    "        x = torch.cat([f_spec, f_iq], dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Helper utils: parameters convertors\n",
    "# ---------------------------\n",
    "def model_to_parameters(model: nn.Module) -> List[np.ndarray]:\n",
    "    params = []\n",
    "    for k, v in model.state_dict().items():\n",
    "        params.append(v.cpu().numpy())\n",
    "    return params\n",
    "\n",
    "\n",
    "def parameters_to_model(model: nn.Module, params: List[np.ndarray]):\n",
    "    state_dict = model.state_dict()\n",
    "    new_state = {}\n",
    "    for (k, _), arr in zip(state_dict.items(), params):\n",
    "        new_state[k] = torch.tensor(arr)\n",
    "    model.load_state_dict(new_state)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Local training function\n",
    "# ---------------------------\n",
    "def train_local(model: nn.Module,\n",
    "                train_loader: DataLoader,\n",
    "                device: torch.device,\n",
    "                epochs: int,\n",
    "                lr: float,\n",
    "                mu: float = 0.0,\n",
    "                global_params: Optional[List[np.ndarray]] = None) -> Tuple[nn.Module, float]: # <-- MODIFIED\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs * len(train_loader), eta_min=1e-6)\n",
    "    use_amp = True if device.type == \"cuda\" else False\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    if mu > 0.0 and global_params is not None:\n",
    "        global_tensors = [torch.tensor(p).to(device) for p in global_params]\n",
    "    else:\n",
    "        global_tensors = None\n",
    "\n",
    "    # --- MODIFIED: Track loss ---\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "    # ----------------------------\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        for batch in train_loader:\n",
    "            iq = batch[\"iq\"].to(device, non_blocking=True)\n",
    "            spec = batch[\"spec\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                outputs = model(iq, spec)\n",
    "                loss = criterion(outputs, labels)\n",
    "                if mu > 0.0 and global_tensors is not None:\n",
    "                    prox_reg = 0.0\n",
    "                    for (k, v), g in zip(model.state_dict().items(), global_tensors):\n",
    "                        prox_reg = prox_reg + torch.sum((v.to(device) - g) ** 2)\n",
    "                    loss = loss + (mu / 2.0) * prox_reg\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # --- MODIFIED: Track loss ---\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_count += labels.size(0)\n",
    "            # ----------------------------\n",
    "\n",
    "    # --- MODIFIED: Return model and avg loss ---\n",
    "    avg_loss = (total_loss / total_count) if total_count > 0 else 0.0\n",
    "    return model, avg_loss\n",
    "    # -------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Local evaluation function\n",
    "# ---------------------------\n",
    "def evaluate_local(model: nn.Module, val_loader: DataLoader, device: torch.device) -> Tuple[float, int, Dict, Dict]:\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    preds_all = []\n",
    "    labels_all = []\n",
    "    snr_all = []\n",
    "    loss_sum = 0.0\n",
    "    n = 0\n",
    "    use_amp = True if device.type == \"cuda\" else False # Added use_amp flag\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            iq = batch[\"iq\"].to(device, non_blocking=True)\n",
    "            spec = batch[\"spec\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"label\"].to(device, non_blocking=True)\n",
    "            snr = batch[\"snr\"]\n",
    "\n",
    "            # Added autocast for evaluation\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                outputs = model(iq, spec)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss_sum += loss.item() * labels.size(0)\n",
    "            n += labels.size(0)\n",
    "            preds_all.append(outputs.argmax(dim=1).cpu().numpy())\n",
    "            labels_all.append(labels.cpu().numpy())\n",
    "            if snr is not None:\n",
    "                # Ensure snr is numpy-compatible\n",
    "                if isinstance(snr, torch.Tensor):\n",
    "                    snr_all.append(snr.cpu().numpy())\n",
    "                elif isinstance(snr, (list, tuple)):\n",
    "                    snr_all.append(np.array(snr))\n",
    "\n",
    "\n",
    "    preds = np.concatenate(preds_all) if preds_all else np.array([])\n",
    "    labels = np.concatenate(labels_all) if labels_all else np.array([])\n",
    "    snr_vals = np.concatenate(snr_all) if snr_all else None\n",
    "    \n",
    "    acc = float(accuracy_score(labels, preds)) if len(labels) > 0 else 0.0\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
    "    \n",
    "    metrics = {\"accuracy\": acc, \"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1)}\n",
    "    results = {\"preds\": preds, \"labels\": labels, \"snr\": snr_vals}\n",
    "    \n",
    "    return (loss_sum / n) if n > 0 else 0.0, n, metrics, results\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Flower client implementation\n",
    "# ---------------------------\n",
    "class FLClient(fl.client.NumPyClient):\n",
    "    # --- MODIFIED: Added attack parameters ---\n",
    "    def __init__(self, cid: str, model: nn.Module, h5_path: str, device: torch.device,\n",
    "                 batch_size: int, downsample: int, local_epochs: int, lr: float, mu: float = 0.0,\n",
    "                 is_malicious: bool = False, attack_alpha: float = 10.0):\n",
    "        self.cid = cid\n",
    "        self.model = model\n",
    "        self.h5_path = h5_path\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.downsample = downsample\n",
    "        self.local_epochs = local_epochs\n",
    "        self.lr = lr\n",
    "        self.mu = mu\n",
    "        \n",
    "        # --- NEW: Store attack flags ---\n",
    "        self.is_malicious = is_malicious\n",
    "        self.attack_alpha = attack_alpha\n",
    "        if self.is_malicious:\n",
    "            print(f\"--- [Client {self.cid}] WARNING: This client is MALICIOUS. (Alpha={self.attack_alpha}) ---\")\n",
    "        # ---------------------------------\n",
    "\n",
    "        spec_transform = T.Compose([\n",
    "            T.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.1, 1.0), value=0),\n",
    "            T.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(1.0, 10.0), value=0)\n",
    "        ])\n",
    "\n",
    "        base_ds = H5RFShard(self.h5_path, downsample=self.downsample, transform=None)\n",
    "        \n",
    "        # We create two instances of the dataset for the split\n",
    "        train_ds_base = H5RFShard(self.h5_path, downsample=self.downsample, transform=spec_transform)\n",
    "        val_ds_base = H5RFShard(self.h5_path, downsample=self.downsample, transform=None)\n",
    "        \n",
    "        n = len(base_ds)\n",
    "        n_train = int(0.9 * n)\n",
    "        n_val = n - n_train\n",
    "\n",
    "        # Split using indices\n",
    "        indices = np.arange(n)\n",
    "        np.random.shuffle(indices)\n",
    "        train_indices = indices[:n_train]\n",
    "        val_indices = indices[n_train:]\n",
    "\n",
    "        # Use Subset to wrap the correct dataset instance\n",
    "        train_set = torch.utils.data.Subset(train_ds_base, train_indices)\n",
    "        val_set = torch.utils.data.Subset(val_ds_base, val_indices)\n",
    "        \n",
    "        self.train_loader = DataLoader(train_set, batch_size=self.batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        self.val_loader = DataLoader(val_set, batch_size=self.batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return model_to_parameters(self.model)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        # --- MODIFIED: Implement Attack Logic ---\n",
    "        \n",
    "        # 1. Store w_t (global model) as tensors\n",
    "        w_t_tensors = [torch.tensor(p).to(self.device) for p in parameters]\n",
    "\n",
    "        # 2. Load w_t into model\n",
    "        parameters_to_model(self.model, parameters)\n",
    "        \n",
    "        # 3. Handle FedProx\n",
    "        global_params = parameters if (self.mu > 0.0) else None\n",
    "        \n",
    "        # 4. Train locally to get w_i' (and avg_loss)\n",
    "        self.model, avg_loss = train_local(\n",
    "            self.model, \n",
    "            self.train_loader, \n",
    "            self.device, \n",
    "            epochs=self.local_epochs, \n",
    "            lr=self.lr,\n",
    "            mu=self.mu, \n",
    "            global_params=global_params\n",
    "        )\n",
    "\n",
    "        # 5. Check if malicious\n",
    "        if self.is_malicious:\n",
    "            print(f\"  [Client {self.cid}] ATTACKING: Reversing and scaling updates...\")\n",
    "            \n",
    "            # Get w_i' (local model) as tensors\n",
    "            w_i_prime_tensors = list(self.model.state_dict().values())\n",
    "            \n",
    "            poisoned_params_list = []\n",
    "            \n",
    "            # Calculate w_poison = w_t - alpha * (w_i' - w_t)\n",
    "            for wt, wip in zip(w_t_tensors, w_i_prime_tensors):\n",
    "                delta = wip.to(self.device) - wt\n",
    "                poison_param = wt - self.attack_alpha * delta\n",
    "                poisoned_params_list.append(poison_param.cpu().numpy())\n",
    "\n",
    "            # Send the poisoned parameters\n",
    "            return poisoned_params_list, len(self.train_loader.dataset), {\"local_loss\": avg_loss, \"attack\": True}\n",
    "        \n",
    "        else:\n",
    "            # Send honest parameters\n",
    "            return model_to_parameters(self.model), len(self.train_loader.dataset), {\"local_loss\": avg_loss, \"attack\": False}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        parameters_to_model(self.model, parameters)\n",
    "        loss, num_examples, metrics, _ = evaluate_local(self.model, self.val_loader, self.device)\n",
    "        \n",
    "        # Add print statement to see local accuracy\n",
    "        print(f\"  [Client {self.cid}] Local validation: Accuracy={metrics['accuracy']:.4f}, Loss={loss:.4f}\")\n",
    "        \n",
    "        return float(loss), int(num_examples), metrics\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Server-side evaluation helpers\n",
    "# ---------------------------\n",
    "def server_evaluate_global(model: nn.Module, test_h5: str, batch_size: int, downsample: int, device: torch.device):\n",
    "    test_ds = H5RFShard(test_h5, downsample=downsample, transform=None) # No augs\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    loss, n, metrics, _ = evaluate_local(model, test_loader, device)\n",
    "    return loss, metrics\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities: plotting metrics\n",
    "# ---------------------------\n",
    "def plot_snr_accuracy(results: Dict, out_dir: str):\n",
    "    if results[\"snr\"] is None or len(results[\"snr\"]) == 0:\n",
    "        print(\"SNR data not found in test set, skipping SNR plot.\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame({\"snr\": results[\"snr\"], \"correct\": (results[\"preds\"] == results[\"labels\"])})\n",
    "    min_snr, max_snr = np.floor(df[\"snr\"].min()), np.ceil(df[\"snr\"].max())\n",
    "    \n",
    "    if min_snr == max_snr:\n",
    "        snr_bins = np.array([min_snr, max_snr + 2])\n",
    "    else:\n",
    "        snr_bins = np.arange(min_snr, max_snr + 2, 2.0)\n",
    "        \n",
    "    df[\"snr_bin\"] = pd.cut(df[\"snr\"], bins=snr_bins, right=False)\n",
    "    \n",
    "    if df[\"snr_bin\"].isnull().all():\n",
    "        print(\"Could not bin SNR data, skipping plot.\")\n",
    "        return\n",
    "\n",
    "    bin_acc = df.groupby(\"snr_bin\", observed=True)[\"correct\"].mean()\n",
    "    bin_counts = df.groupby(\"snr_bin\", observed=True)[\"correct\"].count()\n",
    "    \n",
    "    bin_centers = (snr_bins[:-1] + snr_bins[1:]) / 2\n",
    "    if len(bin_centers) != len(bin_acc):\n",
    "        bin_centers = bin_centers[:len(bin_acc)]\n",
    "\n",
    "    bin_acc_plot = bin_acc.reindex(df[\"snr_bin\"].cat.categories).fillna(0)\n",
    "    bin_counts_plot = bin_counts.reindex(df[\"snr_bin\"].cat.categories).fillna(0)\n",
    "    \n",
    "    if len(bin_centers) != len(bin_acc_plot):\n",
    "        bin_centers = (bin_acc_plot.index.left + bin_acc_plot.index.right) / 2\n",
    "\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax1.plot(bin_centers, bin_acc_plot, 'bo-', label=\"Accuracy\")\n",
    "    ax1.set_xlabel(\"SNR (dB)\"); ax1.set_ylabel(\"Accuracy\", color=\"b\")\n",
    "    ax1.tick_params(axis='y', labelcolor='b'); ax1.set_ylim(0, 1.05); ax1.grid(True, linestyle='--')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.bar(bin_centers, bin_counts_plot, width=1.8, alpha=0.3, color=\"gray\", label=\"Sample Count\")\n",
    "    ax2.set_ylabel(\"Sample Count\", color=\"gray\"); ax2.tick_params(axis='y', labelcolor='gray')\n",
    "\n",
    "    plt.title(\"Accuracy vs. SNR\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"snr_vs_accuracy.png\"))\n",
    "    plt.close()\n",
    "    print(f\"SNR vs. Accuracy plot saved to {os.path.join(out_dir, 'snr_vs_accuracy.png')}\")\n",
    "\n",
    "\n",
    "def plot_confusion_and_report(model: nn.Module, test_h5: str, downsample: int, device: torch.device, out_dir: str):\n",
    "    test_ds = H5RFShard(test_h5, downsample=downsample, transform=None) # No augs\n",
    "    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    model.to(device).eval()\n",
    "    \n",
    "    loss, n, metrics, results = evaluate_local(model, test_loader, device)\n",
    "    \n",
    "    preds = results[\"preds\"]\n",
    "    labels = results[\"labels\"]\n",
    "\n",
    "    print(\"\\n--- Final Global Model Evaluation ---\")\n",
    "    print(f\"Test Loss: {loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    \n",
    "    if len(labels) > 0 and len(preds) > 0:\n",
    "        print(\"\\nClassification report:\\n\", classification_report(labels, preds, digits=4, zero_division=0))\n",
    "        \n",
    "        cm = confusion_matrix(labels, preds)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix (Global Model)\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(out_dir, \"confusion_matrix.png\"))\n",
    "        plt.close()\n",
    "        print(f\"Confusion matrix saved to {os.path.join(out_dir, 'confusion_matrix.png')}\")\n",
    "    else:\n",
    "        print(\"No labels or predictions found, skipping classification report and confusion matrix.\")\n",
    "\n",
    "    plot_snr_accuracy(results, out_dir)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Server-side evaluation/saving function factory\n",
    "# ---------------------------\n",
    "def get_evaluate_fn(model: nn.Module, test_h5: Optional[str], batch_size: int, downsample: int, device: torch.device, out_dir: str, num_rounds: int, num_classes: int):\n",
    "    \n",
    "    best_acc = 0.0 # Track best accuracy\n",
    "    \n",
    "    # We need a model instance *on the server* for evaluation\n",
    "    eval_model = MultiModalNet(num_classes=num_classes).to(device)\n",
    "    \n",
    "    def evaluate(server_round: int,\n",
    "                 parameters: fl.common.NDArrays,\n",
    "                 config: Dict[str, fl.common.Scalar]) -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:\n",
    "        \n",
    "        nonlocal best_acc # Use the outer scope's best_acc\n",
    "        \n",
    "        parameters_to_model(eval_model, parameters) # Load weights into server's eval model\n",
    "        loss, metrics = 0.0, {}\n",
    "\n",
    "        if test_h5 is not None:\n",
    "            # We create the test_loader *inside* the function\n",
    "            test_ds = H5RFShard(test_h5, downsample=downsample, transform=None)\n",
    "            test_loader = DataLoader(test_ds, batch_size=batch_size, num_workers=0)\n",
    "            \n",
    "            loss, n, metrics, _ = evaluate_local(eval_model, test_loader, device)\n",
    "            print(f\"Server-side evaluation round {server_round} / {num_rounds}: Loss {loss:.4f} | Acc {metrics['accuracy']:.4f}\")\n",
    "            \n",
    "            # Save best model logic\n",
    "            if metrics['accuracy'] > best_acc:\n",
    "                best_acc = metrics['accuracy']\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                save_path = os.path.join(out_dir, \"global_model_best.pth\")\n",
    "                torch.save(eval_model.state_dict(), save_path)\n",
    "                print(f\"✅ Best model saved (Acc={best_acc:.4f})\")\n",
    "        else:\n",
    "            print(f\"Federated round {server_round} / {num_rounds} complete.\")\n",
    "\n",
    "        # Save final model on the last round\n",
    "        if server_round == num_rounds:\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            save_path = os.path.join(out_dir, \"global_model_final.pth\")\n",
    "            torch.save(eval_model.state_dict(), save_path)\n",
    "            print(f\"Final global model saved to {save_path}\")\n",
    "\n",
    "        return loss, metrics\n",
    "    return evaluate\n",
    "\n",
    "print(\"All classes and functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12933a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded. Running on device: cuda\n",
      "Output directory set to: ./fl_output_notebook\n",
      "!!! WARNING: POISONING ATTACK ENABLED !!!\n",
      "    -> Malicious Clients: ['0', '1']\n",
      "    -> Attack Alpha: 10.0\n",
      "Training on 5 clients. Using Client 4's data for global evaluation.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# === Cell 2: Configuration ===\n",
    "# ==============================================================================\n",
    "# (This cell replaces all the command-line arguments)\n",
    "\n",
    "NUM_CLIENTS = 5\n",
    "NUM_CLASSES = 7\n",
    "NUM_ROUNDS = 10\n",
    "LOCAL_EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "DOWNSAMPLE = 8\n",
    "LR = 1e-4\n",
    "MU = 0.0 # FedProx coefficient (0.0 = standard FedAvg)\n",
    "\n",
    "# --- [MODIFIED] Attack Configuration ---\n",
    "ATTACK_ENABLED = True\n",
    "MALICIOUS_CLIENT_IDS = [\"0\", \"1\"] # <-- Poison clients 0 and 1\n",
    "ATTACK_ALPHA = 10.0         # Scaling factor (e.g., 10)\n",
    "# -------------------------------------\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "OUT_DIR = \"./fl_output_notebook\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration loaded. Running on device: {DEVICE}\")\n",
    "print(f\"Output directory set to: {OUT_DIR}\")\n",
    "if ATTACK_ENABLED:\n",
    "    print(f\"!!! WARNING: POISONING ATTACK ENABLED !!!\")\n",
    "    print(f\"    -> Malicious Clients: {MALICIOUS_CLIENT_IDS}\")\n",
    "    print(f\"    -> Attack Alpha: {ATTACK_ALPHA}\")\n",
    "\n",
    "# --- !!! IMPORTANT: UPDATE THESE PATHS !!! ---\n",
    "CLIENT_DATA_PATHS = {\n",
    "    \"0\": r\"C:\\\\Users\\\\my pc\\\\Desktop\\\\UAV authentication using federated learning\\\\data\\\\client0.h5\",\n",
    "    \"1\": r\"C:\\\\Users\\\\my pc\\\\Desktop\\\\UAV authentication using federated learning\\\\data\\\\client1.h5\",\n",
    "    \"2\": r\"C:\\\\Users\\\\my pc\\\\Desktop\\\\UAV authentication using federated learning\\\\data\\\\client2.h5\",\n",
    "    \"3\": r\"C:\\\\Users\\\\my pc\\\\Desktop\\\\UAV authentication using federated learning\\\\data\\\\client3.h5\",\n",
    "    \"4\": r\"C:\\\\Users\\\\my pc\\\\Desktop\\\\UAV authentication using federated learning\\\\data\\\\client4.h5\",\n",
    "}\n",
    "\n",
    "# --- Use one of the client files as the test set ---\n",
    "GLOBAL_TEST_H5_PATH = CLIENT_DATA_PATHS[\"4\"]\n",
    "print(f\"Training on {NUM_CLIENTS} clients. Using Client 4's data for global evaluation.\")\n",
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ccec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client factory `client_fn` defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# === Cell 3: Client Factory ===\n",
    "# ==============================================================================\n",
    "# (This function tells Flower's simulation how to create a client)\n",
    "\n",
    "def client_fn(cid: str) -> fl.client.Client:\n",
    "    \"\"\"Create a Flower client instance.\"\"\"\n",
    "    \n",
    "    h5_path = CLIENT_DATA_PATHS[cid]\n",
    "    if not os.path.exists(h5_path):\n",
    "        print(f\"Warning: Data path not found for client {cid}: {h5_path}\")\n",
    "    \n",
    "    model = MultiModalNet(num_classes=NUM_CLASSES)\n",
    "    \n",
    "    # --- [MODIFIED] Check if this client is in the malicious list ---\n",
    "    is_malicious = ATTACK_ENABLED and (cid in MALICIOUS_CLIENT_IDS)\n",
    "    # -------------------------------------------------------------\n",
    "    \n",
    "    client = FLClient(\n",
    "        cid=cid,\n",
    "        model=model,\n",
    "        h5_path=h5_path,\n",
    "        device=DEVICE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        downsample=DOWNSAMPLE,\n",
    "        local_epochs=LOCAL_EPOCHS,\n",
    "        lr=LR,\n",
    "        mu=MU,\n",
    "        # --- Pass attack flags ---\n",
    "        is_malicious=is_malicious,\n",
    "        attack_alpha=ATTACK_ALPHA\n",
    "        # -------------------------------\n",
    "    )\n",
    "    return client.to_client()\n",
    "\n",
    "print(\"Client factory `client_fn` defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e2cc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
      "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
      "\n",
      "\t\t$ flwr new  # Create a new Flower app from a template\n",
      "\n",
      "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
      "\n",
      "\tUsing `start_simulation()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=10, no round_timeout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Flower simulation for 10 rounds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 09:28:41,574\tINFO worker.py:1771 -- Started a local Ray instance.\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 12.0, 'object_store_memory': 1461937766.0, 'node:127.0.0.1': 1.0, 'memory': 2923875534.0, 'GPU': 1.0}\n",
      "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.2}\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 5 actors\n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n",
      "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation round 0 / 10: Loss 2.0173 | Acc 0.0865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 2.017263355301504, {'accuracy': 0.08652043969403779, 'precision': 0.15333964632561828, 'recall': 0.14663322139957652, 'f1': 0.026889485975713306}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best model saved (Acc=0.0865)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:207: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:207: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m   [Client 0] ATTACKING: Reversing and scaling updates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   [Client 1] ATTACKING: Reversing and scaling updates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n",
      "C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (1, nan, {'accuracy': 0.021376829947824326, 'precision': 0.0030538328496891895, 'recall': 0.14285714285714285, 'f1': 0.005979835767069809}, 323.3322797000001)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation round 1 / 10: Loss nan | Acc 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:207: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   [Client 4] Local validation: Accuracy=0.0157, Loss=nan\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[93mWARNING \u001b[0m:   No evaluate_metrics_aggregation_fn provided\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m   [Client 0] ATTACKING: Reversing and scaling updates...\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m   [Client 0] Local validation: Accuracy=0.0223, Loss=nan\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (2, nan, {'accuracy': 0.021376829947824326, 'precision': 0.0030538328496891895, 'recall': 0.14285714285714285, 'f1': 0.005979835767069809}, 633.7107759999926)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:207: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   warnings.warn(\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation round 2 / 10: Loss nan | Acc 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m   [Client 1] ATTACKING: Reversing and scaling updates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m   [Client 4] Local validation: Accuracy=0.0213, Loss=nan\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m   [Client 1] ATTACKING: Reversing and scaling updates...\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m   [Client 3] Local validation: Accuracy=0.0218, Loss=nan\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (3, nan, {'accuracy': 0.021376829947824326, 'precision': 0.0030538328496891895, 'recall': 0.14285714285714285, 'f1': 0.005979835767069809}, 942.2129147000087)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation round 3 / 10: Loss nan | Acc 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:207: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m   warnings.warn(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   [Client 0] ATTACKING: Reversing and scaling updates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   [Client 4] Local validation: Accuracy=0.0278, Loss=nan\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m   [Client 0] ATTACKING: Reversing and scaling updates...\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m   [Client 1] Local validation: Accuracy=0.0223, Loss=nan\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (4, nan, {'accuracy': 0.021376829947824326, 'precision': 0.0030538328496891895, 'recall': 0.14285714285714285, 'f1': 0.005979835767069809}, 1248.6624122000067)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation round 4 / 10: Loss nan | Acc 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:207: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   warnings.warn(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m   [Client 1] ATTACKING: Reversing and scaling updates...\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   [Client 4] Local validation: Accuracy=0.0187, Loss=nan\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m   [Client 0] ATTACKING: Reversing and scaling updates...\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m   [Client 1] Local validation: Accuracy=0.0213, Loss=nan\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (5, nan, {'accuracy': 0.021376829947824326, 'precision': 0.0030538328496891895, 'recall': 0.14285714285714285, 'f1': 0.005979835767069809}, 1567.1197182000033)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation round 5 / 10: Loss nan | Acc 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:207: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   warnings.warn(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   [Client 1] ATTACKING: Reversing and scaling updates...\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m   [Client 4] Local validation: Accuracy=0.0233, Loss=nan\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 6]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   [Client 1] ATTACKING: Reversing and scaling updates...\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m   [Client 1] Local validation: Accuracy=0.0213, Loss=nan\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (6, nan, {'accuracy': 0.021376829947824326, 'precision': 0.0030538328496891895, 'recall': 0.14285714285714285, 'f1': 0.005979835767069809}, 1877.3924300000072)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation round 6 / 10: Loss nan | Acc 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:207: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m   warnings.warn(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m   [Client 0] ATTACKING: Reversing and scaling updates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m   [Client 4] Local validation: Accuracy=0.0203, Loss=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 7]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   [Client 0] ATTACKING: Reversing and scaling updates...\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   [Client 0] Local validation: Accuracy=0.0284, Loss=nan\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (7, nan, {'accuracy': 0.021376829947824326, 'precision': 0.0030538328496891895, 'recall': 0.14285714285714285, 'f1': 0.005979835767069809}, 2173.885219699994)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation round 7 / 10: Loss nan | Acc 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:207: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m   warnings.warn(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m   [Client 1] ATTACKING: Reversing and scaling updates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m   [Client 4] Local validation: Accuracy=0.0187, Loss=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 8]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m   [Client 0] ATTACKING: Reversing and scaling updates...\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m   [Client 0] Local validation: Accuracy=0.0208, Loss=nan\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (8, nan, {'accuracy': 0.021376829947824326, 'precision': 0.0030538328496891895, 'recall': 0.14285714285714285, 'f1': 0.005979835767069809}, 2469.096992100007)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation round 8 / 10: Loss nan | Acc 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:207: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m   warnings.warn(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   [Client 1] ATTACKING: Reversing and scaling updates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m   [Client 4] Local validation: Accuracy=0.0172, Loss=nan\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 9]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   [Client 0] ATTACKING: Reversing and scaling updates...\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m   [Client 1] Local validation: Accuracy=0.0223, Loss=nan\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (9, nan, {'accuracy': 0.021376829947824326, 'precision': 0.0030538328496891895, 'recall': 0.14285714285714285, 'f1': 0.005979835767069809}, 2766.9072826000047)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation round 9 / 10: Loss nan | Acc 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:207: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m   warnings.warn(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m   [Client 1] ATTACKING: Reversing and scaling updates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m   [Client 4] Local validation: Accuracy=0.0203, Loss=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 10]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m   [Client 1] ATTACKING: Reversing and scaling updates...\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m   [Client 0] Local validation: Accuracy=0.0213, Loss=nan\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation round 10 / 10: Loss nan | Acc 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      fit progress: (10, nan, {'accuracy': 0.021376829947824326, 'precision': 0.0030538328496891895, 'recall': 0.14285714285714285, 'f1': 0.005979835767069809}, 3057.4336729999923)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final global model saved to ./fl_output_notebook\\global_model_final.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=2360)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:207: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m c:\\Users\\my pc\\.conda\\envs\\fedai\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m   warnings.warn(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32880)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m --- [Client 0] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n",
      "\u001b[36m(ClientAppActor pid=21176)\u001b[0m   [Client 0] ATTACKING: Reversing and scaling updates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=32092)\u001b[0m   [Client 4] Local validation: Accuracy=0.0218, Loss=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 10 round(s) in 3063.42s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 3: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 4: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 5: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 6: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 7: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 8: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 9: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 10: nan\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, centralized):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 0: 2.017263355301504\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 3: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 4: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 5: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 6: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 7: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 8: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 9: nan\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 10: nan\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, centralized):\n",
      "\u001b[92mINFO \u001b[0m:      \t{'accuracy': [(0, 0.08652043969403779),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (1, 0.021376829947824326),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (2, 0.021376829947824326),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (3, 0.021376829947824326),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (4, 0.021376829947824326),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (5, 0.021376829947824326),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (6, 0.021376829947824326),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (7, 0.021376829947824326),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (8, 0.021376829947824326),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (9, 0.021376829947824326),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (10, 0.021376829947824326)],\n",
      "\u001b[92mINFO \u001b[0m:      \t 'f1': [(0, 0.026889485975713306),\n",
      "\u001b[92mINFO \u001b[0m:      \t        (1, 0.005979835767069809),\n",
      "\u001b[92mINFO \u001b[0m:      \t        (2, 0.005979835767069809),\n",
      "\u001b[92mINFO \u001b[0m:      \t        (3, 0.005979835767069809),\n",
      "\u001b[92mINFO \u001b[0m:      \t        (4, 0.005979835767069809),\n",
      "\u001b[92mINFO \u001b[0m:      \t        (5, 0.005979835767069809),\n",
      "\u001b[92mINFO \u001b[0m:      \t        (6, 0.005979835767069809),\n",
      "\u001b[92mINFO \u001b[0m:      \t        (7, 0.005979835767069809),\n",
      "\u001b[92mINFO \u001b[0m:      \t        (8, 0.005979835767069809),\n",
      "\u001b[92mINFO \u001b[0m:      \t        (9, 0.005979835767069809),\n",
      "\u001b[92mINFO \u001b[0m:      \t        (10, 0.005979835767069809)],\n",
      "\u001b[92mINFO \u001b[0m:      \t 'precision': [(0, 0.15333964632561828),\n",
      "\u001b[92mINFO \u001b[0m:      \t               (1, 0.0030538328496891895),\n",
      "\u001b[92mINFO \u001b[0m:      \t               (2, 0.0030538328496891895),\n",
      "\u001b[92mINFO \u001b[0m:      \t               (3, 0.0030538328496891895),\n",
      "\u001b[92mINFO \u001b[0m:      \t               (4, 0.0030538328496891895),\n",
      "\u001b[92mINFO \u001b[0m:      \t               (5, 0.0030538328496891895),\n",
      "\u001b[92mINFO \u001b[0m:      \t               (6, 0.0030538328496891895),\n",
      "\u001b[92mINFO \u001b[0m:      \t               (7, 0.0030538328496891895),\n",
      "\u001b[92mINFO \u001b[0m:      \t               (8, 0.0030538328496891895),\n",
      "\u001b[92mINFO \u001b[0m:      \t               (9, 0.0030538328496891895),\n",
      "\u001b[92mINFO \u001b[0m:      \t               (10, 0.0030538328496891895)],\n",
      "\u001b[92mINFO \u001b[0m:      \t 'recall': [(0, 0.14663322139957652),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22440)\u001b[0m --- [Client 1] WARNING: This client is MALICIOUS. (Alpha=10.0) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \t            (1, 0.14285714285714285),\n",
      "\u001b[92mINFO \u001b[0m:      \t            (2, 0.14285714285714285),\n",
      "\u001b[92mINFO \u001b[0m:      \t            (3, 0.14285714285714285),\n",
      "\u001b[92mINFO \u001b[0m:      \t            (4, 0.14285714285714285),\n",
      "\u001b[92mINFO \u001b[0m:      \t            (5, 0.14285714285714285),\n",
      "\u001b[92mINFO \u001b[0m:      \t            (6, 0.14285714285714285),\n",
      "\u001b[92mINFO \u001b[0m:      \t            (7, 0.14285714285714285),\n",
      "\u001b[92mINFO \u001b[0m:      \t            (8, 0.14285714285714285),\n",
      "\u001b[92mINFO \u001b[0m:      \t            (9, 0.14285714285714285),\n",
      "\u001b[92mINFO \u001b[0m:      \t            (10, 0.14285714285714285)]}\n",
      "\u001b[92mINFO \u001b[0m:      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation finished.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# === Cell 4: Run the Simulation (Training) ===\n",
    "# ==============================================================================\n",
    "# (This cell starts and runs the entire federated training process)\n",
    "# (No changes needed, but added OOM fix)\n",
    "\n",
    "# We need a model instance on the \"server\" for saving the final model\n",
    "server_model = MultiModalNet(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Create the server-side function (it will just save the model, no per-round eval)\n",
    "eval_fn = get_evaluate_fn(\n",
    "    model=server_model,\n",
    "    test_h5=GLOBAL_TEST_H5_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    downsample=DOWNSAMPLE,\n",
    "    device=DEVICE,\n",
    "    out_dir=OUT_DIR,\n",
    "    num_rounds=NUM_ROUNDS,\n",
    "    num_classes=NUM_CLASSES # Pass num_classes\n",
    ")\n",
    "\n",
    "# Define the strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,          # Train on all 5 clients\n",
    "    fraction_evaluate=1.0,     # Evaluate on all 5 clients' local validation sets\n",
    "    min_fit_clients=NUM_CLIENTS,\n",
    "    min_evaluate_clients=NUM_CLIENTS,\n",
    "    min_available_clients=NUM_CLIENTS,\n",
    "    evaluate_fn=eval_fn  # This will save the model on the last round\n",
    ")\n",
    "\n",
    "# --- [NEW] Add client_resources to prevent OOM errors ---\n",
    "ray_init_args = {\"num_gpus\": 1}\n",
    "# 0.20 * 5 clients = 1.0 GPU\n",
    "client_resources = {\"num_cpus\": 1, \"num_gpus\": 0.20} \n",
    "# --------------------------------------------------------\n",
    "\n",
    "print(f\"Starting Flower simulation for {NUM_ROUNDS} rounds...\")\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),\n",
    "    strategy=strategy,\n",
    "    # --- [NEW] Pass resources ---\n",
    "    client_resources=client_resources,\n",
    "    ray_init_args=ray_init_args\n",
    "    # ----------------------------\n",
    ")\n",
    "print(\"Simulation finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c86a6489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Final Evaluation ---\n",
      "Loading best model from: ./fl_output_notebook\\global_model_best.pth\n",
      "Evaluating on data from: C:\\\\Users\\\\my pc\\\\Desktop\\\\UAV authentication using federated learning\\\\data\\\\client4.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\my pc\\AppData\\Local\\Temp\\ipykernel_17600\\2272618123.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Global Model Evaluation ---\n",
      "Test Loss: 2.0173\n",
      "Test Accuracy: 0.0865\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       422\n",
      "           1     0.0734    1.0000    0.1367      1428\n",
      "           2     0.0000    0.0000    0.0000       690\n",
      "           3     0.0000    0.0000    0.0000      1281\n",
      "           4     1.0000    0.0263    0.0513     10593\n",
      "           5     0.0000    0.0000    0.0000      3246\n",
      "           6     0.0000    0.0000    0.0000      2081\n",
      "\n",
      "    accuracy                         0.0865     19741\n",
      "   macro avg     0.1533    0.1466    0.0269     19741\n",
      "weighted avg     0.5419    0.0865    0.0374     19741\n",
      "\n",
      "Confusion matrix saved to ./fl_output_notebook\\confusion_matrix.png\n",
      "SNR vs. Accuracy plot saved to ./fl_output_notebook\\snr_vs_accuracy.png\n",
      "\n",
      "Evaluation complete. Plots saved to ./fl_output_notebook\n",
      "\n",
      "Plotting training history...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Extract local loss\u001b[39;00m\n\u001b[0;32m     45\u001b[0m local_loss_hist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 46\u001b[0m rounds \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics_distributed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rounds:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rounds:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'fit'"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# === Cell 5: Evaluate the Final Model ===\n",
    "# ==============================================================================\n",
    "# (This cell loads the saved model and runs a full evaluation)\n",
    "# (No changes needed)\n",
    "\n",
    "# --- Configuration for evaluation ---\n",
    "MODEL_PATH = os.path.join(OUT_DIR, \"global_model_best.pth\") # Load best model\n",
    "EVAL_H5_PATH = CLIENT_DATA_PATHS[\"4\"] \n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "print(f\"\\n--- Starting Final Evaluation ---\")\n",
    "print(f\"Loading best model from: {MODEL_PATH}\")\n",
    "print(f\"Evaluating on data from: {EVAL_H5_PATH}\")\n",
    "\n",
    "# 1. Initialize model\n",
    "final_model = MultiModalNet(num_classes=NUM_CLASSES)\n",
    "\n",
    "# 2. Load saved weights\n",
    "try:\n",
    "    final_model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    final_model.to(DEVICE)\n",
    "\n",
    "    # 3. Run evaluation and plotting\n",
    "    plot_confusion_and_report(\n",
    "        model=final_model, \n",
    "        test_h5=EVAL_H5_PATH, \n",
    "        downsample=DOWNSAMPLE, \n",
    "        device=DEVICE, \n",
    "        out_dir=OUT_DIR\n",
    "    )\n",
    "    print(f\"\\nEvaluation complete. Plots saved to {OUT_DIR}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file not found at {MODEL_PATH}.\")\n",
    "    print(\"Please ensure the simulation in 'Cell 4' ran successfully and saved a model.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during evaluation: {e}\")\n",
    "\n",
    "# --- Plot Training History ---\n",
    "if 'history' in locals():\n",
    "    print(\"\\nPlotting training history...\")\n",
    "    # Extract local loss\n",
    "    local_loss_hist = []\n",
    "    rounds = history.metrics_distributed[\"fit\"].keys()\n",
    "    \n",
    "    if rounds:\n",
    "        for r in rounds:\n",
    "            round_losses = [metrics.get(\"local_loss\", 0.0) for cid, metrics in history.metrics_distributed[\"fit\"][r]]\n",
    "            if round_losses:\n",
    "                avg_loss = np.mean(round_losses)\n",
    "                local_loss_hist.append(avg_loss)\n",
    "            else:\n",
    "                local_loss_hist.append(np.nan)\n",
    "\n",
    "        # Extract global accuracy\n",
    "        if \"evaluate\" in history.metrics_centralized and history.metrics_centralized[\"evaluate\"]:\n",
    "            global_acc_hist = [metrics[\"accuracy\"] for r, metrics in history.metrics_centralized[\"evaluate\"]]\n",
    "            \n",
    "            plt.figure(figsize=(10,4))\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.plot(rounds, local_loss_hist, marker='o')\n",
    "            plt.title(\"Avg Client Loss per Round\")\n",
    "            plt.xlabel(\"Round\"); plt.ylabel(\"Loss\"); plt.grid(True)\n",
    "\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.plot(rounds, global_acc_hist, marker='o', color='green')\n",
    "            plt.title(\"Global Test Accuracy per Round\")\n",
    "            plt.xlabel(\"Round\"); plt.ylabel(\"Accuracy\"); plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No centralized evaluation metrics found in history.\")\n",
    "    else:\n",
    "        print(\"No fit metrics found in history.\")\n",
    "else:\n",
    "    print(\"No 'history' object found. Skipping training plots.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
